#Modes de transmission
- Transmission séquentielle
Par défaut : balayage (scan line)

- Transmission progressive
	- lignes impaires suivies de lignes paires
	- par plan de bits
	- entrelacement
		- Adam 7 : 7 passes

		| 1 |   | 4 |   | 2 |   | 4 |   |
|---|---|---|---|---|---|---|---|
| 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 |
|   |   |   |   |   |   |   |   |
| 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 |
| 3 |   | 4 |   | 3 |   | 4 |   |
| 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 |
|   |   |   |   |   |   |   |   |
| 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 |

	- entrelacement pyramidal par bloc

#Théorie de l'information

##Quantité d'information

***Définitions, propriétés : principes de la compression des données***

**Claude Shannon**

- Théorème fondamental de Shannon

> Pour une source d'information donnée, ou le canal d'information donné, il existe toujours un code permettant de transmettre le message en utilisant la capacité de canal, avec un taux d'erreur binaire (TEB, ou BER(en)) fixé.
 
- Signal : séquence d'évenements finie ou non
- **Alphabet** 
- nombre d'evenements : longueur
	- texte : lettres
	- signal numérique : bits, octets
	- image : pixels (256)
	- son : pressions
	- son numérique : échantillons
- Tous les évènements sont identiques -> *source constante*
- Tous les évenements sont indépendants -> *source aléatoire*
- Si la production d'un evt est condition des précédents -> *evenements corelés* -> source *Markovienne* d'ordre $n$. Si le signal est fini : $n$ = longueur du message. **Extraction de corrélations**

##Mesure d'information
Information $\rightarrow$ probabilité

L'information transmise : choix fait par la source entre un certain nombres d'evenements + ou - probables.

**Source finie :** probabilité et occurrence : nombre d'apparition de chaque symbole de l'alphabet
	- source : m evts
	- symbole $\alpha$ : k fois
	- $O(\alpha) = k$ occurence
	- $p(\alpha) = \frac{k}{m}$ probabilité

Information : $\rightarrow$ relative à la source
		$\rightarrow$ indépendante de la taille de l'alphabet

>*Ex :* S = "compression_de_données"
>22 evts, alphabet min = 11 symboles
>$p(e) = \frac{4}{22}$ $\sum p(\alpha_{i}) = 1$ 

**Information :** l'information véhiculée par *un evt* est inversement proportionnelle à son occurence. La probabilité diminue $\rightarrow$ information augmente

- L'information globale véhiculée par un ensemble d'evts indépendants = à la somme des informations de chaque evt pris indépendamment.
- L'information globale véhiculée par un ensemble d'evts dépendants < à la somme des informations de chaque evt pris indépendamment.


> $I(\alpha_{i})=\log(\frac{1}{p(\alpha_{i})})$
> $= -\log(p(\alpha_{i}))$

$\log_{2} \rightarrow$ signaux binaires $\rightarrow$ bits

Source binaire : $p(0) = p(1) = \frac{1}{2}$

$I(0) = I(1) = -\log_{2}{\frac{1}{2}} = 1$bits

*Ex:* image 8 niveaux de gris de taille 500x600 pixels
$N=8^{3.10^{5}}$
1 image parmi $N^{image \neq}$
$I($une image$) = \log_{2}{8^{3.10^{5}}}$
texte : vocabulaire de 100 000 mots, message de 1000 mots
$I($message$) = 1000\log_{2}{10^{5}} \approx 2.10^{4}$bits

**Signal aléatoire**
alphabet N symboles
$p(\alpha_{i}) = \frac{1}{N} \rightarrow I(\alpha_{i}) = \log(N)$

**Source Markovienne**
$I(\alpha_{i}) \ll \log(N)$ ; $I(\alpha_{j}) \ll \log(N)$